## Budget Categories → Account Presets Consolidation Plan

### Context
- `budget_categories` is a standalone table with FK references, offline caching, and CRUD endpoints.
- `account_presets` already stores other account-level defaults inside the `presets` JSON blob.
- Maintaining both patterns produces duplicate cache layers, diverging RLS policies, and inconsistent onboarding.

### Objectives
- Single canonical store for account-scoped presets (`account_presets`).
- Preserve existing ergonomics: categories still behave like relational rows with IDs.
- No data loss; transactions referencing category IDs remain valid.
- Keep offline-first behavior and Supabase security semantics intact.
- Documented validation metrics with clear green / yellow / red criteria so any engineer can judge rollout safety.

### Target Architecture
- Embed categories under `account_presets.presets.budget_categories` as an ordered array of objects `{ id, name, slug, is_archived, metadata, created_at, updated_at }`.
- Expose read/write compatibility via a Postgres view + RPC layer (`vw_budget_categories`) so existing services can select/insert as if the table still exists.
- Category IDs stay as UUIDs; transactions continue storing the UUID.
- All preset data (default category, ordering, categories) is cached via a single offline payload.

### Migration Phases
1. **Preparation**
   - Confirm every account has an `account_presets` row; create ones that are missing.
   - Document the new contract in `dev_docs` and align with engineering leads.
   - Define the monitoring surface before touching data by authoring `dev_docs/metrics/budget_categories_monitoring.md`. That spec must include:
     - Schema plan for a new Supabase table `budget_category_parity_metrics` (or equivalent storage) that records `account_id`, count diff, checksum match flag, and timestamp.
     - Requirements for `scripts/metrics/publish_budget_category_parity.ts`, a CLI that queries both sources, writes to the table, and emits a Markdown summary file (`dev_docs/metrics/budget_categories_report.md`). This Markdown report becomes the deterministic “dashboard” AI devs consult; no external tooling is assumed.
2. **Dual-Write Enablement**
   - Extend `accountPresetsService` with helpers to get/set embedded categories.
   - Update `budgetCategoriesService` to write to both the table and the embedded blob (feature-flagged).
   - Add observability (logs + metrics) comparing table vs blob counts per account; include:
     - `category_count_diff` gauge (table count minus blob count).
     - `category_checksum_match` boolean (MD5/sha hash equality).
     - `last_dual_write_timestamp` to detect stalled writers.
   - Page the owning squad if diff ≠ 0 for >15 minutes on any environment.
3. **Data Backfill**
   - Supabase SQL migration to aggregate each account’s categories, sorted by existing `budget_category_order`, and store them inside `account_presets`.
   - Set a checksum column (e.g., MD5 of serialized object) to validate parity during rollout.
   - Record per-account backfill status in `account_presets.backfill_state` (`pending`, `complete`, `needs_retry`) so the parity report script can filter out partial work.
4. **Read Switch**
   - Create `vw_budget_categories` (selects from `account_presets`, unnesting the JSON array) and a `rpc_upsert_budget_category` for writes.
   - Flip the app to read via the view (or through the expanded service) while still persisting to the table for fallback.
   - Run automated parity checks; block rollout if mismatches occur. A phase cannot progress unless:
     - 0 checksum mismatches in staging for 48h.
     - <0.1% of production accounts have non-zero `category_count_diff` for 7 consecutive days.
     - The parity report generated by `scripts/metrics/publish_budget_category_parity.ts` shows no regressed error rates in `accountPresetsService` (error budget delta ≤ previous phase).
5. **Retirement**
   - Freeze table writes (RLS deny insert/update) once the above metrics stay green in production for a full release cycle (minimum 14 days).
   - Update `transactions.category_id` FK to reference a generated table or trigger that validates IDs against the JSON payload.
   - Drop the legacy table and clean up code paths, feature flags, and caches.
   - Leave the parity metrics pipeline + Markdown report generation in place for one more release so regressions can still be caught post-drop.

### Schema & Supabase Work
- Migration scripts:
  - `01_ensure_account_presets.sql` – backfill missing rows.
  - `02_embed_budget_categories.sql` – populate JSON payload, set checksum.
  - `03_create_budget_category_view.sql` – define view + helper functions with appropriate RLS.
  - `04_adjust_transactions_fk.sql` – new constraint strategy once the table is retired.
- RLS:
  - Mirror existing `budget_categories` policies on the view/RPC functions.
  - Ensure `account_presets` policies allow updates to `presets -> budget_categories` only by owners.

### Application Changes
- **Service layer**
  - Add serialization/deserialization utilities in `accountPresetsService`.
  - Refactor offline cache to hydrate categories from the consolidated payload; drop duplicate cache keys.
  - Introduce telemetry comparing counts between sources and emit the metrics listed under Dual-Write Enablement.
- **Feature flagging**
  - `BUDGET_CATEGORIES_EMBEDDED_READS`
  - `BUDGET_CATEGORIES_EMBEDDED_WRITES`
  - `BUDGET_CATEGORIES_TABLE_DISABLED`
  - Use gradual rollout (internal → beta → everyone). As part of this project:
    - Create `dev_docs/feature_flags/budget_categories.md` that restates the metrics in this plan plus the explicit go/no-go checklist. That file becomes the machine-readable “register” the automation reads.
    - Build `scripts/feature-flags/toggle.ts` (or extend existing tooling) so an AI dev invokes a single command `pnpm toggle-flag <flag> <env> <on|off>`. The script must refuse to run if the register file or required metrics snapshot is missing.
    - The rollout procedure for AI devs is then deterministic: read the register entry → run the script → verify it prints the metrics snapshot and records success/failure. If the prerequisites are absent, the script exits with “BLOCKED – missing register/metrics”, preventing improvisation.

### Testing Strategy
- Unit tests for new serialization helpers and dual-write logic. Include fixture parity tests that fail if serialization order changes.
- Integration tests hitting Supabase functions to exercise the view/RPC.
- Dry-run definition: execute the full Supabase migration/backfill workflow **without** touching production by using a sanitized production export in a disposable staging database. The dry-run must catch schema errors, data-shape mismatches, and checksum differences before we roll anything out.
- Dry-runs against sanitized production exports: as part of this project, add `scripts/backfill/create_sanitized_snapshot.ts`. The job, when invoked, pulls the latest prod backup, scrubs PII, and loads it into a disposable staging database; run the migrations/backfill, compare row counts, ID sets, and serialized hashes, then append the results to the Markdown parity report for traceability. The CI pipeline must execute this script **every** time a dry-run is triggered; if the script fails (no fresh backup, scrub step error, staging load failure), the job exits with “BLOCKED – snapshot creation failed” and nothing proceeds.
- Offline regression tests: ensure category cache warms from the new payload and updates propagate while offline.
- Add a scripted smoke test (`scripts/verify-budget-category-parity.ts`) that engineers can run locally to sanity check staging before flag flips.

### Rollback Plan
- Keep dual-writes enabled until the table is dropped so we can repopulate either side quickly.
- Store migration checkpoints (per-account hashes) so we can re-sync the table from `account_presets` if needed.
- Maintain feature flags for at least one release post-migration to revert reads back to the legacy table.
- Document the manual steps for replaying the backfill and point to the Markdown parity report so on-call can validate success without hunting through logs.

### Ownership & Timeline
- **Data layer**: Supabase migrations + view/RPC work (data engineering).
- **App layer**: Service refactors + feature flags (application team).
- **QA**: Offline + regression testing (QA automation team).
- Suggested timeline: 1 sprint for dual-write + backfill, 1 sprint for read switch + validation, 1 sprint for retirement/cleanup.
